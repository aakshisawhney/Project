# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LarcLoWi2mnakJvKuYZijFiKy_rYUG2R
"""

!pip install transformers datasets

from datasets import load_dataset

dataset = load_dataset("jfleg")
print(dataset)

print("Original:", dataset['validation'][0]['sentence'])
print("Corrected:", dataset['validation'][0]['corrections'][0])

from transformers import T5Tokenizer

tokenizer = T5Tokenizer.from_pretrained("t5-small")

max_len = 128

def preprocess(example):
    input_text = "correct grammar: " + example['sentence']
    target_text = example['corrections'][0]

    inputs = tokenizer(input_text, max_length=max_len, truncation=True, padding="max_length")
    targets = tokenizer(target_text, max_length=max_len, truncation=True, padding="max_length")

    return {
        'input_ids': inputs.input_ids,
        'attention_mask': inputs.attention_mask,
        'labels': targets.input_ids
    }

tokenized_dataset = dataset.map(preprocess, remove_columns=['sentence', 'corrections'])

from transformers import T5ForConditionalGeneration

model = T5ForConditionalGeneration.from_pretrained("t5-small")

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./t5_grammar_correction",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=3e-4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['validation'], # Use validation for training
    eval_dataset=tokenized_dataset['test'], # Use test for evaluation
    tokenizer=tokenizer,
)

trainer.train()

def correct_grammar(text):
    input_text = "correct grammar: " + text
    inputs = tokenizer(input_text, return_tensors="pt", max_length=128, truncation=True)
    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)
    corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return corrected

# Try correcting a sentence
print(correct_grammar("She go to school yesterday."))

